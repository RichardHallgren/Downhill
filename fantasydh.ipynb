{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter venue and date to predict here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df = pd.read_csv('Points.csv', sep=';')\n",
    "points_dict = dict(zip(points_df.Rank, points_df.Points))\n",
    "\n",
    "#ENTER VENUE AND DATE HERE\n",
    "    \n",
    "ven_pred = 'Lousa'\n",
    "date_pred = '20200322'\n",
    "\n",
    "#ENTER RIDERS NOT PARTICIPATING IN RACE HERE\n",
    "np_riders = ['Martin MAES']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes of riders whos results should be predicted\n",
    "\n",
    "Some riders are not participating in all races, this could be because of injuries, or they are riders that won't participate in all world cups for various reasons (for example Martin Maes is an enduro rider who only sometimes (rarely) participate in downhill events, therefore he's removed from the participating riders). The create_prediction_df creates dataframes containing only the riders that should be included in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_df():\n",
    "\n",
    "    \n",
    "    comp_df = pd.read_csv('riderprices2020.csv', sep=';')\n",
    "    comp_df_m = comp_df[(comp_df.Gender == 'M') & (comp_df.Injured == 'No') & (~comp_df.Name.isin(np_riders))].copy()\n",
    "    comp_df_w = comp_df[(comp_df.Gender == 'W') & (comp_df.Injured == 'No')].copy()\n",
    "\n",
    "    comp_dfs = [comp_df_m, comp_df_w]  \n",
    "    \n",
    "    for df in comp_dfs:\n",
    "        \n",
    "        df.drop(['Price', 'Injured', 'Gender'], axis=1, inplace=True)\n",
    "        df['Venue'] = ven_pred\n",
    "        df['Date'] = pd.to_datetime(date_pred, format='%Y%m%d')\n",
    "        df['Year'] = df.Date.dt.year\n",
    "\n",
    "    return (comp_df_m, comp_df_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes\n",
    "\n",
    "The function create_results_from_UCI_df creates a dataframe from excel-files containing results downloaded from the UCI (Union Cycliste Internationale) Website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_from_UCI_df(path):\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    files_xlsx = [f for f in files if f[-4:] == 'xlsx']\n",
    "    filepaths = [path + '/' + file for file in files_xlsx]\n",
    "\n",
    "    udf = pd.DataFrame()\n",
    "    for f in filepaths:\n",
    "        data = pd.read_excel(f)\n",
    "        stripped_f = f.rstrip('.xlsx').replace('Results_UCI_M/', '').replace('Results_UCI_W/', '')\n",
    "        split_f = stripped_f.split('_')\n",
    "        data['Venue'] = split_f[0]\n",
    "        data['Category'] = split_f[1]\n",
    "        data['Date'] = split_f[2]\n",
    "        data['Name'] = data['First Name'] + ' ' + data['Last Name']\n",
    "        data['Date'] = pd.to_datetime(data['Date'], format='%Y%m%d')\n",
    "        data.drop(['First Name', 'Last Name', 'Phase', 'Heat', 'IRM', 'Team', 'Gender', 'Result', 'Country', 'Category', 'BIB'], axis=1, inplace=True)\n",
    "        udf = udf.append(data, sort=False)\n",
    "        udf.dropna(subset=['Rank'], inplace=True)\n",
    "        udf.Rank = udf.Rank.astype(int)\n",
    "        udf = udf[(udf.Rank < 81)]\n",
    "        udf.sort_values(by=['Date', 'Rank'], inplace=True)\n",
    " \n",
    "        #Create points feature\n",
    "        udf['Points'] = udf.Rank.map(points_dict)\n",
    "        udf.Points.fillna(0, inplace=True)\n",
    "    \n",
    "        #Create year feature\n",
    "        udf['Year'] = udf.Date.dt.year\n",
    "    \n",
    "    return udf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function create_results_from_scraped_web_data creates a dataframe from the results scraped from a separate downhill website (see the separate Scraped_web_data_ETL.ipynb for more info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_from_scraped_web_data():\n",
    "    scraped_df = pd.read_excel('race_dfs_output.xlsx')\n",
    "    \n",
    "    #change name errors\n",
    "    name_dict = {'Sam HILL' : 'Samuel HILL', 'Mick HANNAH' : 'Michael HANNAH'}\n",
    "    venue_dict = {'Fort-William' : 'Fortwilliam', 'Les-Gets' : 'Lesgets', 'Les-Deux-Alpes' : 'Lesdeuxalpes'}\n",
    "    scraped_df.Name.replace(name_dict, inplace=True)\n",
    "    scraped_df.Venue.replace(venue_dict, inplace=True)\n",
    "    scraped_df['Date'] = pd.to_datetime(scraped_df['Date'], format='%Y%m%d')\n",
    "    \n",
    "    #Create points feature\n",
    "    scraped_df['Points'] = scraped_df.Rank.map(points_dict)\n",
    "    scraped_df.Points.fillna(0, inplace=True)\n",
    "    scraped_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    scraped_df = scraped_df[(scraped_df.Rank < 81)]\n",
    "\n",
    "\n",
    "    scraped_df.sort_values(by=['Date', 'Rank'], inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "    return scraped_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features\n",
    "The create_features and findpreviouswin functions creates 5 features based on earlier results the rider has had. These features are chosen because of the assumption that prior results from the rider will be an indicator of future results. Especially more recent results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(cfdf):\n",
    "    #Create Moving Average over last 3 races\n",
    "    cfdf['MA3 Pos'] = cfdf.groupby('Name')['Rank'].transform(lambda x: x.rolling(3, 1).mean().shift())\n",
    "\n",
    "    #Create last race position feature\n",
    "    cfdf['Last RP'] = cfdf.groupby('Name')['Rank'].transform(lambda x: x.rolling(1, 1).mean().shift())\n",
    "\n",
    "    #Best position in the last 5 races\n",
    "    cfdf['Best pos'] = cfdf.groupby('Name')['Rank'].transform(lambda x: x.rolling(5, 1).min().shift())\n",
    "\n",
    "    #Average position current season\n",
    "    cfdf['AP this year'] = cfdf.groupby(['Name', 'Year'])['Rank'].transform(lambda x: x.rolling(10, 1).mean().shift())\n",
    "\n",
    "    #Number of races rider has participated in current season\n",
    "    #cfdf['Races CS'] = cfdf.groupby(['Name', 'Year'])['Rank'].transform(lambda x: x.rolling(10, 1).count().shift())\n",
    "    #cfdf['Races CS'].fillna(0, inplace=True)\n",
    "    \n",
    "    return cfdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findpreviouswin(compdate, compyear, name, df):  \n",
    "    # Function to create a feature for the riders average position the previous year\n",
    "    \n",
    "    avgpos_pyear = df[(df.Name == name) & (df.Year == compyear-1)].Rank.mean()\n",
    "    \n",
    "    return pd.Series({'AP last season' : avgpos_pyear})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fantasy model\n",
    "\n",
    "To avoid data leakage the last race will be hold out from the training data instead of holding out random samples from the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fantasy_model(dfinp):\n",
    "    \n",
    "    # Set dev_model to 1 to test the model before training on entire dataset. \n",
    "    # dev_model = 0 means that the model will take all data into account before predicting the\n",
    "    # upcoming race\n",
    "    dev_model = 0\n",
    "    \n",
    "    #preprocessing of dataframe\n",
    "    \n",
    "    #train and test set to evaluate performance of model before training model on all data and using it to predict\n",
    "    #future competitions\n",
    "    \n",
    "    if dev_model == 1:\n",
    "    \n",
    "        df_test = dfinp[(dfinp['Venue'] == 'Snowshoe') & (dfinp['Date'] == '2019-09-06')].copy()\n",
    "        df_train = dfinp[(dfinp['Venue'] != 'Snowshoe') & (dfinp['Date'] != '2019-09-06') & (dfinp.Rank < 81)].copy()\n",
    "    \n",
    "    \n",
    "    #create df to train model on all data before using it to predict upcoming competitions, excluding upcoming competition\n",
    "    \n",
    "    if dev_model == 0:\n",
    "    \n",
    "        df_train = dfinp[(dfinp['Venue'] != ven_pred) & (dfinp['Date'] != date_pred) & (dfinp.Rank < 81)].copy()\n",
    "        #df_pred is the dataframe containing the venue to be predicted\n",
    "        df_test = dfinp[(dfinp['Venue'] == ven_pred) & (dfinp['Date'] == pd.to_datetime(date_pred, format='%Y%m%d'))].copy()\n",
    "\n",
    "\n",
    "    \n",
    "    y_train = df_train.Points\n",
    "    df_train.drop('Points', axis=1, inplace=True)\n",
    "    y_test = df_test.Points\n",
    "    df_test.drop('Points', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Preprocessing for numerical data\n",
    "    numerical_transformer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    numerical_cols = ['Age', 'Year', 'MA3 Pos', 'Last RP', 'Best pos', 'AP this year', 'AP last season']\n",
    "    categorical_cols = ['Venue', 'Name']\n",
    "    \n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "     ])\n",
    "    \n",
    "    \n",
    "    # Keep selected columns only\n",
    "    my_cols = categorical_cols + numerical_cols\n",
    "    X_train = df_train[my_cols].copy()\n",
    "    #X_test = df_pred[my_cols].copy()\n",
    "    X_test = df_test[my_cols].copy()\n",
    "    \n",
    "    eval_set = [(df_test, y_test)]\n",
    "    \n",
    "    model = XGBRegressor(n_estimators=1800, learning_rate=0.001, n_jobs=4, max_depth=5, \n",
    "                         objective=\"reg:squarederror\", eval_set = eval_set\n",
    "                        ,early_stopping_rounds=100, eval_metric='mae')\n",
    "    \n",
    "\n",
    "    \n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)])\n",
    "    \n",
    "\n",
    "    #scores = cross_val_score(my_pipeline, X_train, y,\n",
    "    #                          cv=10,\n",
    "    #                          scoring='neg_mean_absolute_error')\n",
    "    #print('Neg MAE:', scores)\n",
    "    #print(\"NEG MAE mean:\", scores.mean())\n",
    "    #print('Standard deviation:', scores.std())\n",
    "    \n",
    "    # Preprocessing of training data, fit model \n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict points distribution for the race event\n",
    "    test_pred = my_pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    mae = mean_absolute_error(y_test, test_pred)\n",
    "    \n",
    "    print('MAE:', mae)\n",
    "    \n",
    "    return pd.Series(test_pred, df_test.Name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def race_prediction_men():\n",
    "    #enter names of result folders\n",
    "    path1 = 'Results_UCI_M'\n",
    "    rdf1 = create_results_from_UCI_df(path1)\n",
    "    \n",
    "    scraped_df = create_results_from_scraped_web_data()\n",
    "\n",
    "    result_df_M = scraped_df.append(rdf1, sort=True)\n",
    "        \n",
    "\n",
    "    comp_df_m = create_prediction_df()[0]\n",
    "    \n",
    "    result_df_M = result_df_M.append(comp_df_m, sort=True)\n",
    "\n",
    "    \n",
    "    #create dataframe with features\n",
    "    mdf = create_features(result_df_M)\n",
    "\n",
    "    #create feature with average number points per race for the previous season\n",
    "    mdf[['AP last season']] = mdf.apply(lambda row: findpreviouswin(row.Date, row.Year, row.Name, mdf), axis=1)\n",
    "\n",
    "    mdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    #Create correlation plot over the dataframe with numerical features\n",
    "    #f, ax = plt.subplots(figsize=(10, 8))\n",
    "    #corr = mdf.corr()\n",
    "    #sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "    #        square=True, ax=ax)\n",
    "    \n",
    "    return fantasy_model(mdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 17.708703247706094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name\n",
       "Danny HART                     87.091087\n",
       "Amaury PIERRON                113.033897\n",
       "Charlie HARRISON               39.832825\n",
       "Loic BRUNI                    109.481789\n",
       "Greg MINNAAR                   67.027321\n",
       "Troy BROSNAN                   93.122063\n",
       "Greg WILLIAMSON                29.191748\n",
       "Thomas ESTAQUE                 29.345692\n",
       "Dakotah NORTON                 24.994755\n",
       "Dean LUCAS                     41.754486\n",
       "Loris VERGIER                  78.745804\n",
       "Laurie GREENLAND               91.374176\n",
       "Aaron GWIN                     62.711662\n",
       "Luca SHAW                      38.432251\n",
       "Connor FEARON                  43.772614\n",
       "Bruce KLEIN                    28.693590\n",
       "Baptiste PIERRON               37.665695\n",
       "Rémi THIRION                   41.396042\n",
       "Martin MAES                    85.639343\n",
       "Jure ŽABJEK                    33.243626\n",
       "Mark WALLACE                   40.659214\n",
       "David TRUMMER                  46.926147\n",
       "Kirk MCDOWALL                  17.564997\n",
       "Loris REVELLI                  15.374117\n",
       "Michael HANNAH                 22.844660\n",
       "Bernard KERR                   33.101875\n",
       "Charlie HATTON                 30.324497\n",
       "Jackson FREW                   13.935597\n",
       "Florent PAYET                  33.918941\n",
       "Andreas KOLB                   24.810448\n",
       "Gee ATHERTON                   37.522430\n",
       "Alex MARIN                     25.413462\n",
       "Marcelo GUTIERREZ VILLEGAS     18.071886\n",
       "Joe BREEDEN                    21.440407\n",
       "Rudy CABIROU                   15.279293\n",
       "Oliver ZWAR                    14.527140\n",
       "Hugo LANGEVIN                  12.708275\n",
       "Kenta GALLAGHER                14.527140\n",
       "Jack MOIR                      32.594387\n",
       "Hugo FRIXTALON                 29.625702\n",
       "Faustin FIGARET                19.308779\n",
       "Max HARTENSTERN                24.593901\n",
       "Benoit COULANGES               28.423407\n",
       "Matthew WALKER                 29.375639\n",
       "Jacob DICKSON                  29.022526\n",
       "Samuel BLENKINSOP              22.897108\n",
       "Basil WEBER                    12.990009\n",
       "Reece WILSON                   42.142681\n",
       "Jack READING                   18.041084\n",
       "Charly DIPASQUALE              15.411414\n",
       "Lutz WEBER                     17.315201\n",
       "Joseph SMITH                   21.827347\n",
       "Matthew SIMMONDS               16.126801\n",
       "Alexandre FAYOLLE              15.172482\n",
       "Gaëtan VIGE                    17.622776\n",
       "George BRANNIGAN               19.662470\n",
       "Gaëtan RUFFIN                  14.527140\n",
       "Brendan FAIRCLOUGH             40.480289\n",
       "Harry MOLLOY                   11.128950\n",
       "Samuel THIBAULT                 8.335232\n",
       "dtype: float32"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_prediction_men()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_prediction_women():\n",
    "    #enter names of result folders\n",
    "    path1 = 'Results_UCI_W'\n",
    "    rdf1 = create_results_from_UCI_df(path1)\n",
    "    #path2 = 'Results_Web_M'\n",
    "    #rdf2 = create_results_from_web_df(path2)\n",
    "    #result_df_M = rdf2.append(rdf1, sort=True)\n",
    "\n",
    "    \n",
    "    comp_df_w = create_prediction_df()[1]\n",
    "\n",
    "    result_df_W = rdf1.append(comp_df_w, sort=True)\n",
    "\n",
    "\n",
    "\n",
    "    #create dataframe with features\n",
    "    wdf = create_features(result_df_W)\n",
    "\n",
    "    #create feature with average number points per race for the previous season\n",
    "    wdf[['AP last season']] = wdf.apply(lambda row: findpreviouswin(row.Date, row.Year, row.Name, wdf), axis=1)\n",
    "\n",
    "    \n",
    "    return fantasy_model(wdf)\n",
    "#race_prediction_women()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rider selection\n",
    "All riders have a set price (this price does not update during the season). The function rider_selection reads in a dataframe containing the prices and the predictions for the races. It uses these dataframes to calculate how much points per dollar each rider is expected to return. This function will be used when finding the best possible rider combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rider_selection(gender):\n",
    "    rider_price_df = pd.read_csv('riderprices2020.csv', sep=';')\n",
    "    \n",
    "    \n",
    "    if gender == 'M':\n",
    "        predictions = race_prediction_men()\n",
    "    else:\n",
    "        predictions = race_prediction_women()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    pri_df = pd.DataFrame(predictions).reset_index()\n",
    "    pri_df = pri_df.rename(columns={0: \"Points\"})\n",
    "\n",
    "    pri_df = pri_df.merge(rider_price_df, left_on='Name', right_on='Name', how='left')\n",
    "    pri_df['Pts./k$'] = pri_df.Points / pri_df.Price*1000\n",
    "    #print(pri_df.head(50))\n",
    "    \n",
    "    if gender == 'M':\n",
    "        pri_df = pri_df.groupby('Price').nth(list(range(4))).reset_index()\n",
    "    else:\n",
    "        pri_df = pri_df.groupby('Price').nth(list(range(2))).reset_index()\n",
    "    \n",
    "    \n",
    "    return pri_df[(pri_df['Pts./k$'] > 0.1)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best possible combination of riders\n",
    "The goal is to have the best possible fantasy downhill team, consisting of a maximum of 6 riders. Maximum 2 women riders and maximum 4 male riders.\n",
    "\n",
    "This function, find_rider_combinations, iterates through all the combinations of 4 Male riders and 2 Female riders and fins the combination that has the highest expected total number of points within the team budget of 1 500 000 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rider_combinations():\n",
    "    import itertools    \n",
    "    from itertools import combinations\n",
    "    \n",
    "    #dataframe with riders in the men category\n",
    "    ridercm_df = rider_selection('M')\n",
    "    \n",
    "    #dataframe with riders in the women category\n",
    "    ridercw_df = rider_selection('W')\n",
    "    #Create all possible combinations of riders\n",
    "    #mask for non participating riders\n",
    "    #nonpart = (riderc_df.Name != \"Martin MAES\")\n",
    "    #rcomb_df = pd.DataFrame.from_records(list(itertools.combinations(riderc_df[nonpart].Name, 4)), columns=['R1', 'R2', 'R3', 'R4'])\n",
    "    \n",
    "    \n",
    "    #Create two separate dataframes for men and women. \n",
    "    #Create combinations of 4 male riders and combinations of 2 female riders.\n",
    "    \n",
    "    rcombm_df = pd.DataFrame.from_records(list(itertools.combinations(ridercm_df.Name, 4)), columns=['MR1', 'MR2', 'MR3', 'MR4'])\n",
    "\n",
    "    #Dictionaries of rider & price/points\n",
    "    rpdict = dict(zip(ridercm_df.Name,ridercm_df.Price))\n",
    "    rptdict = dict(zip(ridercm_df.Name,ridercm_df.Points))\n",
    "\n",
    "    rcombm_df['CPriceM'] = rcombm_df.MR1.map(rpdict) + rcombm_df.MR2.map(rpdict) + rcombm_df.MR3.map(rpdict) + rcombm_df.MR4.map(rpdict)\n",
    "    rcombm_df['CPointsM'] = rcombm_df.MR1.map(rptdict) + rcombm_df.MR2.map(rptdict) + rcombm_df.MR3.map(rptdict) + rcombm_df.MR4.map(rptdict)\n",
    "    rcombm_df['Comb. Pts/k$'] = rcombm_df.CPointsM / rcombm_df.CPriceM * 1000\n",
    "\n",
    "    rcombm_df.sort_values(by=['CPointsM'], ascending = False, inplace=True)\n",
    "    rcombm_df['RidersM'] = list(zip(rcombm_df.MR1, rcombm_df.MR2, rcombm_df.MR3, rcombm_df.MR4))\n",
    "\n",
    "    #rcombm_df.drop(rcombm_df[rcombm_df.CPriceM > 50000].index, inplace=True)\n",
    "\n",
    "    rcombm_df = rcombm_df[(rcombm_df.CPriceM < 1500000) & (rcombm_df.CPriceM > 340000)].copy()\n",
    "    \n",
    "    #Creation of dataframes for female riders\n",
    "    rcombw_df = pd.DataFrame.from_records(list(itertools.combinations(ridercw_df.Name, 2)), columns=['WR1', 'WR2'])\n",
    "    \n",
    "    rpwdict = dict(zip(ridercw_df.Name,ridercw_df.Price))\n",
    "    rptwdict = dict(zip(ridercw_df.Name,ridercw_df.Points))\n",
    "    \n",
    "    rcombw_df['CPriceW'] = rcombw_df.WR1.map(rpwdict) + rcombw_df.WR2.map(rpwdict) \n",
    "    rcombw_df['CPointsW'] = rcombw_df.WR1.map(rptwdict) + rcombw_df.WR2.map(rptwdict) \n",
    "    rcombw_df['Comb. Pts/k$ W'] = rcombw_df.CPointsW / rcombw_df.CPriceW * 1000\n",
    "\n",
    "    rcombw_df.sort_values(by=['CPointsW'], ascending = False, inplace=True)\n",
    "    rcombw_df['RidersW'] = list(zip(rcombw_df.WR1, rcombw_df.WR2))\n",
    "\n",
    "    #find best combinations of both men and women\n",
    "    combinate_df = pd.DataFrame.from_records(list(itertools.product(rcombw_df.RidersW, rcombm_df.RidersM)), columns=['RidersW', 'RidersM'])\n",
    "    \n",
    "    \n",
    "    combinate_df = combinate_df.merge(rcombw_df, left_on='RidersW', right_on='RidersW')\n",
    "    combinate_df = combinate_df.merge(rcombm_df, left_on='RidersM', right_on='RidersM')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    combinate_df['Combined Price'] = combinate_df.CPriceW + combinate_df.CPriceM\n",
    "    combinate_df['Combined Points'] = combinate_df.CPointsW + combinate_df.CPointsM\n",
    "\n",
    "    combinate_df.drop(['MR1', 'MR2', 'MR3', 'MR4', 'WR1', 'WR2', 'CPriceW', 'CPriceM', 'CPointsW', 'CPointsM'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    combinate_df.sort_values(by=['Combined Points'], ascending = False, inplace=True)\n",
    "    \n",
    "    return combinate_df[(combinate_df['Combined Price'] < 1500001)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 17.708703247706094\n",
      "MAE: 22.31589916774205\n"
     ]
    }
   ],
   "source": [
    "# Display the entire column width\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Find the optimal rider combinations\n",
    "find_rider_combinations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
